{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad4b5af-767a-4152-ab1e-6e29c4b54c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(name=\"env\",defaultValue=\"\",label=\"Enter the environment in lower case\")\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    lit, avg, col, row_number, when, stddev, lag, sum as spark_sum, max as spark_max\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import date_sub, current_date\n",
    "\n",
    "\n",
    "input_table = f\"{env}_silver.daily_price_aggregates\"\n",
    "output_table = f\"{env}_gold.daily_price_indicators\"\n",
    "windows = [60,26,12]\n",
    "\n",
    "def read_prices(input_table):\n",
    "\n",
    "    # Calculate start date 90 days ago (Python side)\n",
    "    start_date = (datetime.today() - timedelta(days=90)).date()\n",
    "\n",
    "    # Extract year and month from start_date\n",
    "    start_year = start_date.year\n",
    "    start_month = start_date.month\n",
    "\n",
    "    # Also get current year and month\n",
    "    current_year = datetime.today().year\n",
    "    current_month = datetime.today().month\n",
    "\n",
    "    df_recent = spark.table(input_table) \\\n",
    "        .filter(\n",
    "            ( (col(\"year\") == start_year) & (col(\"month\") >= start_month) ) |\n",
    "            ( (col(\"year\") > start_year) & (col(\"year\") < current_year) ) |\n",
    "            ( (col(\"year\") == current_year) & (col(\"month\") <= current_month) )\n",
    "        )\n",
    "\n",
    "    df_filtered = df_recent.filter(col(\"date\") >= lit(start_date.strftime(\"%Y-%m-%d\")))\n",
    "    window_spec = Window.partitionBy(\"symbol\").orderBy(col(\"date\").desc())\n",
    "    df_latest = df_filtered.withColumn(\"rn\", row_number().over(window_spec))\n",
    "\n",
    "    return df_latest\n",
    "\n",
    "\n",
    "def moving_average(df_input, df_output, window_sizes):\n",
    "    ma_dfs = []\n",
    "    for ws in window_sizes:\n",
    "        valid_symbols = (\n",
    "            df_input.groupBy(\"symbol\")\n",
    "            .agg(spark_max(\"rn\").alias(\"max_rn\"))\n",
    "            .filter(col(\"max_rn\") >= ws)\n",
    "            .select(\"symbol\")\n",
    "        )\n",
    "        df_filtered = df_input.join(valid_symbols, on=\"symbol\", how=\"inner\")\n",
    "\n",
    "        ma_df = df_filtered.groupBy(\"symbol\").agg(\n",
    "            avg(when(col(\"rn\") <= ws, col(\"avg_price\"))).alias(f\"ma_{ws}\")\n",
    "        )\n",
    "        ma_dfs.append(ma_df)\n",
    "\n",
    "    # Join all MAs on symbol\n",
    "    if ma_dfs:\n",
    "        ma_all = reduce(lambda left, right: left.join(right, on=\"symbol\", how=\"outer\"), ma_dfs)\n",
    "    else:\n",
    "        ma_all = df_input.select(\"symbol\").distinct()\n",
    "\n",
    "    df_output = df_output.join(ma_all, on=\"symbol\", how=\"left\")\n",
    "    return df_output\n",
    "\n",
    "\n",
    "def take_latest_row(table_path, granulity):\n",
    "\n",
    "    if granulity == \"daily\":\n",
    "        datecol = \"date\"\n",
    "    elif granulity == \"hourly\":\n",
    "        datecol = \"timestamp\"\n",
    "\n",
    "    five_days_ago = date_sub(current_date(), 5)\n",
    "    df_recent = (\n",
    "        spark.read.table(table_path)\n",
    "        .filter(col(datecol) >= five_days_ago)  # Adjust to your date column\n",
    "    )\n",
    "\n",
    "    # Define window: partition by symbol, order by date descending\n",
    "    window_spec = Window.partitionBy(\"symbol\").orderBy(col(datecol).desc())\n",
    "\n",
    "    # Assign row number to get the latest record per symbol\n",
    "    df_latest = (\n",
    "        df_recent\n",
    "        .withColumn(\"rn\", row_number().over(window_spec))\n",
    "        .filter(col(\"rn\") == 1)\n",
    "        .drop(\"rn\")\n",
    "    )\n",
    "    return df_latest\n",
    "\n",
    "\n",
    "def exponential_moving_average(latest_records, df_output, window_sizes):\n",
    "    # Step 1: Select relevant EMA columns from latest_records and rename to avoid ambiguity\n",
    "    ema_cols = [f\"ema_{ws}\" for ws in window_sizes]\n",
    "    existing_cols = [c for c in ema_cols if c in latest_records.columns]\n",
    "\n",
    "    latest_selected = latest_records.select(\n",
    "        col(\"symbol\"),\n",
    "        *[col(c).alias(f\"prev_{c}\") for c in existing_cols]\n",
    "    )\n",
    "\n",
    "    # Step 2: Join historical EMA values to today's output\n",
    "    joined_df = df_output.join(latest_selected, on=\"symbol\", how=\"left\")\n",
    "\n",
    "    # Step 3: Calculate each EMA using the smoothing formula\n",
    "    for ws in window_sizes:\n",
    "        k = 2 / (ws + 1)\n",
    "        ema_col = f\"ema_{ws}\"\n",
    "        prev_col = f\"prev_{ema_col}\"\n",
    "        ma_col = f\"ma_{ws}\"\n",
    "\n",
    "        joined_df = joined_df.withColumn(\n",
    "            ema_col,\n",
    "            when(\n",
    "                col(prev_col).isNull(),  # First day fallback to MA\n",
    "                col(ma_col)\n",
    "            ).otherwise(\n",
    "                col(prev_col) * (1 - k) + col(\"avg_price\") * k\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Step 4: Select calculated EMAs\n",
    "    result_cols = [\"symbol\"] + [f\"ema_{ws}\" for ws in window_sizes]\n",
    "    ema_today = joined_df.select(*result_cols)\n",
    "\n",
    "    # Step 5: Merge into df_output\n",
    "    df_output = df_output.join(ema_today, on=\"symbol\", how=\"left\")\n",
    "\n",
    "    return df_output\n",
    "\n",
    "\n",
    "def calculate_rsi(df_input, df_output, window_sizes):\n",
    "    rsi_dfs = []\n",
    "\n",
    "    for ws in window_sizes:\n",
    "        win_spec = Window.partitionBy(\"symbol\").orderBy(col(\"date\").desc())\n",
    "\n",
    "        # Calculate price difference (delta)\n",
    "        df_with_delta = df_input.withColumn(\"price_diff\", col(\"avg_price\") - lag(\"avg_price\", 1).over(win_spec))\n",
    "\n",
    "        # Calculate gain and loss\n",
    "        df_with_gains = df_with_delta.withColumn(\"gain\", when(col(\"price_diff\") > 0, col(\"price_diff\")).otherwise(0))\n",
    "        df_with_gains = df_with_gains.withColumn(\"loss\", when(col(\"price_diff\") < 0, -col(\"price_diff\")).otherwise(0))\n",
    "\n",
    "        # Only keep rows where we have enough history\n",
    "        df_valid = df_with_gains.filter(col(\"rn\") <= ws)\n",
    "\n",
    "        # Aggregate total gains and losses over the window\n",
    "        rsi_df = df_valid.groupBy(\"symbol\").agg(\n",
    "            (spark_sum(\"gain\") / ws).alias(\"avg_gain\"),\n",
    "            (spark_sum(\"loss\") / ws).alias(\"avg_loss\")\n",
    "        )\n",
    "\n",
    "        # Calculate RSI\n",
    "        rsi_df = rsi_df.withColumn(\n",
    "            f\"rsi_{ws}\",\n",
    "            when(\n",
    "                col(\"avg_loss\") == 0, 100.0\n",
    "            ).otherwise(\n",
    "                100 - (100 / (1 + (col(\"avg_gain\") / col(\"avg_loss\"))))\n",
    "            )\n",
    "        ).select(\"symbol\", f\"rsi_{ws}\")\n",
    "\n",
    "        rsi_dfs.append(rsi_df)\n",
    "\n",
    "    # Join all RSI outputs on symbol\n",
    "    if rsi_dfs:\n",
    "        rsi_all = reduce(lambda left, right: left.join(right, on=\"symbol\", how=\"outer\"), rsi_dfs)\n",
    "    else:\n",
    "        rsi_all = df_input.select(\"symbol\").distinct()\n",
    "\n",
    "    df_output = df_output.join(rsi_all, on=\"symbol\", how=\"left\")\n",
    "\n",
    "    return df_output\n",
    "    \n",
    "\n",
    "\n",
    "def calculate_macd(latest_records, df_output):\n",
    "    # Step 1: Get EMA_12 and EMA_26 from previous output (rename to avoid ambiguity)\n",
    "    macd_required_emas = [\"ema_12\", \"ema_26\", \"macd\", \"signal\"]\n",
    "    existing_cols = [col_name for col_name in macd_required_emas if col_name in latest_records.columns]\n",
    "\n",
    "    latest_selected = latest_records.select(\n",
    "        col(\"symbol\"),\n",
    "        *[col(c).alias(f\"prev_{c}\") for c in existing_cols]\n",
    "    )\n",
    "\n",
    "    # Step 2: Join with today's base data (avg_price already in df_output)\n",
    "    df_joined = df_output.join(latest_selected, on=\"symbol\", how=\"left\")\n",
    "\n",
    "    # EMA smoothing constants\n",
    "    k_12 = 2 / (12 + 1)\n",
    "    k_26 = 2 / (26 + 1)\n",
    "    k_9 = 2 / (9 + 1)\n",
    "\n",
    "    # Step 3: Calculate EMA_12\n",
    "    df_joined = df_joined.withColumn(\n",
    "        \"ema_12\",\n",
    "        when(col(\"prev_ema_12\").isNull(), col(\"ma_12\"))\n",
    "        .otherwise(col(\"prev_ema_12\") * (1 - k_12) + col(\"avg_price\") * k_12)\n",
    "    )\n",
    "\n",
    "    # Step 4: Calculate EMA_26\n",
    "    df_joined = df_joined.withColumn(\n",
    "        \"ema_26\",\n",
    "        when(col(\"prev_ema_26\").isNull(), col(\"ma_26\"))\n",
    "        .otherwise(col(\"prev_ema_26\") * (1 - k_26) + col(\"avg_price\") * k_26)\n",
    "    )\n",
    "\n",
    "    # Step 5: Calculate MACD Line\n",
    "    df_joined = df_joined.withColumn(\"macd\", col(\"ema_12\") - col(\"ema_26\"))\n",
    "\n",
    "    # Step 6: Calculate Signal Line (EMA_9 of MACD)\n",
    "    df_joined = df_joined.withColumn(\n",
    "        \"signal\",\n",
    "        when(col(\"prev_signal\").isNull(), col(\"macd\"))\n",
    "        .otherwise(col(\"prev_signal\") * (1 - k_9) + col(\"macd\") * k_9)\n",
    "    )\n",
    "\n",
    "    # Step 7: Histogram\n",
    "    df_joined = df_joined.withColumn(\"macd_hist\", col(\"macd\") - col(\"signal\"))\n",
    "\n",
    "    # Step 8: Select only relevant columns for update\n",
    "    macd_cols = [\"symbol\", \"ema_12\", \"ema_26\", \"macd\", \"signal\", \"macd_hist\"]\n",
    "    macd_df = df_joined.select(macd_cols)\n",
    "\n",
    "    # Step 9: Merge into df_output\n",
    "    df_output = df_output.join(macd_df, on=\"symbol\", how=\"left\")\n",
    "\n",
    "    return df_output\n",
    "\n",
    "\n",
    "def calculate_bollinger_bands(df_input, df_output, window_sizes):\n",
    "    bb_dfs = []\n",
    "\n",
    "    for ws in window_sizes:\n",
    "        # Only calculate BB for symbols with at least `ws` days\n",
    "        valid_symbols = (\n",
    "            df_input.groupBy(\"symbol\")\n",
    "            .agg(spark_max(\"rn\").alias(\"max_rn\"))\n",
    "            .filter(col(\"max_rn\") >= ws)\n",
    "            .select(\"symbol\")\n",
    "        )\n",
    "        df_filtered = df_input.join(valid_symbols, on=\"symbol\", how=\"inner\")\n",
    "\n",
    "        # Compute average and stddev of avg_price over last `ws` days\n",
    "        bb_df = df_filtered.groupBy(\"symbol\").agg(\n",
    "            avg(when(col(\"rn\") <= ws, col(\"avg_price\"))).alias(f\"bb_ma_{ws}\"),\n",
    "            stddev(when(col(\"rn\") <= ws, col(\"avg_price\"))).alias(f\"bb_std_{ws}\")\n",
    "        )\n",
    "\n",
    "        # Calculate upper and lower bands\n",
    "        bb_df = bb_df.withColumn(f\"bb_upper_{ws}\", col(f\"bb_ma_{ws}\") + 2 * col(f\"bb_std_{ws}\"))\n",
    "        bb_df = bb_df.withColumn(f\"bb_lower_{ws}\", col(f\"bb_ma_{ws}\") - 2 * col(f\"bb_std_{ws}\"))\n",
    "        bb_df = bb_df.withColumn(f\"bb_width_{ws}\", col(f\"bb_upper_{ws}\") - col(f\"bb_lower_{ws}\"))\n",
    "\n",
    "        # Select relevant columns\n",
    "        bb_df = bb_df.select(\"symbol\", f\"bb_upper_{ws}\", f\"bb_lower_{ws}\", f\"bb_width_{ws}\")\n",
    "        bb_dfs.append(bb_df)\n",
    "\n",
    "    # Join all BB results\n",
    "    if bb_dfs:\n",
    "        bb_all = reduce(lambda left, right: left.join(right, on=\"symbol\", how=\"outer\"), bb_dfs)\n",
    "    else:\n",
    "        bb_all = df_input.select(\"symbol\").distinct()\n",
    "\n",
    "    df_output = df_output.join(bb_all, on=\"symbol\", how=\"left\")\n",
    "\n",
    "    return df_output\n",
    "\n",
    "\n",
    "def calculate_obv(df_input, df_output):\n",
    "\n",
    "    # Create a window by symbol ordered by date\n",
    "    win = Window.partitionBy(\"symbol\").orderBy(\"date\")\n",
    "\n",
    "    # Compute price difference\n",
    "    df_with_delta = df_input.withColumn(\"price_diff\", col(\"avg_price\") - lag(\"avg_price\", 1).over(win))\n",
    "\n",
    "    # Determine OBV adjustment: +volume if price up, -volume if price down, 0 otherwise\n",
    "    df_with_direction = df_with_delta.withColumn(\n",
    "        \"obv_change\",\n",
    "        when(col(\"price_diff\") > 0, col(\"volume\"))\n",
    "        .when(col(\"price_diff\") < 0, -col(\"volume\"))\n",
    "        .otherwise(0)\n",
    "    )\n",
    "\n",
    "    # OBV is cumulative sum of obv_change per symbol\n",
    "    df_with_obv = df_with_direction.withColumn(\"obv\", spark_sum(\"obv_change\").over(win.rowsBetween(Window.unboundedPreceding, Window.currentRow)))\n",
    "\n",
    "    # Keep only the latest OBV (rn == 1)\n",
    "    latest_obv = df_with_obv.filter(col(\"rn\") == 1).select(\"symbol\", \"obv\")\n",
    "\n",
    "    # Join into result\n",
    "    df_output = df_output.join(latest_obv, on=\"symbol\", how=\"left\")\n",
    "\n",
    "    return df_output\n",
    "\n",
    "\n",
    "df_filtered = read_prices(input_table)\n",
    "df_filtered.cache()\n",
    "\n",
    "df_result = df_filtered.filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "#Moving averages\n",
    "df_result = moving_average(df_filtered, df_result, windows)\n",
    "#Exponential moving averages\n",
    "latest_records = take_latest_row(output_table, \"daily\")\n",
    "df_result = exponential_moving_average(latest_records, df_result, windows)\n",
    "#RSI\n",
    "df_result = calculate_rsi(df_filtered, df_result, windows)\n",
    "#MACD\n",
    "df_result = calculate_macd(latest_records, df_result)\n",
    "#Bollinger bands\n",
    "df_result = calculate_bollinger_bands(df_filtered, df_result, windows)\n",
    "#OBV\n",
    "df_result = calculate_obv(df_filtered, df_result)\n",
    "\n",
    "pandas_df = df_result.limit(100).toPandas()\n",
    "display(pandas_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3160f002-598c-493e-a9e7-3b3481d13053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM hive_metastore.dev_gold.daily_price_indicators\")\n",
    "pandas_df = df.limit(100).toPandas()\n",
    "display(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72b935aa-d14b-4861-8a98-02b157670139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_techMeasures_daily",
   "widgets": {
    "env": {
     "currentValue": "dev",
     "nuid": "10a4e55c-2959-49dd-a039-4843a2424b61",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter the environment in lower case",
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter the environment in lower case",
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
