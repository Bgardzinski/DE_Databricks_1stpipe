{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6fb1d02-cc8d-4738-911f-aa523b42fbcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/bartoszgardzinski1@gmail.com/DE_Databricks_1stpipe/utils/technical_indicators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad4b5af-767a-4152-ab1e-6e29c4b54c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(name=\"env\",defaultValue=\"\",label=\"Enter the environment in lower case\")\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "\n",
    "#hello\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    lit, avg, col, row_number, when, stddev, lag, sum as spark_sum, max as spark_max\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import date_sub, current_date\n",
    "import time\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "input_table = f\"{env}_silver.daily_price_aggregates\"\n",
    "output_table = f\"{env}_gold.daily_price_indicators\"\n",
    "output_columns = ['symbol', 'date', 'open', 'day_high', 'day_low', 'avg_price', 'volume', 'year', 'month', 'day', 'ma_12', 'ma_26', 'ma_60', 'ema_12', 'ema_26', 'ema_60', 'rsi_12', 'rsi_26', 'rsi_60', 'bb_upper_12', 'bb_lower_12', 'bb_width_12', 'bb_upper_26', 'bb_lower_26', 'bb_width_26', 'bb_upper_60', 'bb_lower_60', 'bb_width_60', 'macd', 'signal', 'macd_hist', 'obv']\n",
    "windows = [60,26,12]\n",
    "\n",
    "def readStream_prices():\n",
    "    return spark.readStream.table(f\"{env}_silver.daily_price_aggregates\")\n",
    "\n",
    "def read_prices(input_table, row_date):\n",
    "\n",
    "    row_date = row_date.date()\n",
    "\n",
    "    if granulity == \"daily\":\n",
    "        datecol = \"date\"\n",
    "        nb_days = 90\n",
    "    elif granulity == \"hourly\":\n",
    "        datecol = \"timestamp\"\n",
    "        nb_days = 5\n",
    "\n",
    "    start_date = (row_date - timedelta(days=nb_days)).date()\n",
    "    end_date = row_date\n",
    "\n",
    "    month_pairs = []\n",
    "    current = start_date.replace(day=1)\n",
    "    while current <= end_date:\n",
    "        month_pairs.append((current.year, current.month))\n",
    "        if current.month == 12:\n",
    "            current = current.replace(year=current.year + 1, month=1)\n",
    "        else:\n",
    "            current = current.replace(month=current.month + 1)\n",
    "\n",
    "    df_recent = spark.table(input_table) \\\n",
    "        .filter((col(\"year\"), col(\"month\")).isin(month_pairs)) \\\n",
    "        .filter((col(\"date\") >= lit(start_date)) & (col(\"date\") <= lit(end_date)))\n",
    "\n",
    "    window_spec = Window.partitionBy(\"symbol\").orderBy(col(datecol).desc())\n",
    "    df_latest = df_filtered.withColumn(\"rn\", row_number().over(window_spec))\n",
    "\n",
    "    return df_latest\n",
    "\n",
    "\n",
    "def take_latest_row(table_path, row_date, granulity):\n",
    "\n",
    "    if granulity == \"daily\":\n",
    "        datecol = \"date\"\n",
    "    elif granulity == \"hourly\":\n",
    "        datecol = \"timestamp\"\n",
    "\n",
    "    row_date = row_date.date()\n",
    "\n",
    "    five_days_ago = date_sub(row_date, 5)\n",
    "    df_recent = (\n",
    "        spark.read.table(table_path)\n",
    "        .filter(col(datecol) >= five_days_ago & col(datecol) < row_date)  # Adjust to your date column\n",
    "    )\n",
    "\n",
    "    # Define window: partition by symbol, order by date descending\n",
    "    window_spec = Window.partitionBy(\"symbol\").orderBy(col(datecol).desc())\n",
    "\n",
    "    # Assign row number to get the latest record per symbol\n",
    "    df_latest = (\n",
    "        df_recent\n",
    "        .withColumn(\"rn\", row_number().over(window_spec))\n",
    "        .filter(col(\"rn\") == 1)\n",
    "        .drop(\"rn\")\n",
    "    )\n",
    "\n",
    "    for c in df_latest.columns:\n",
    "        if c != \"symbol\":\n",
    "            df_latest = df_latest.withColumnRenamed(c, f\"prev_{c}\")\n",
    "\n",
    "    return df_latest\n",
    "\n",
    "def run_all_indicators(df_filtered, df_result, windows):\n",
    "\n",
    "    df_result = moving_average(df_filtered, df_result, windows)\n",
    "\n",
    "    df_result = exponential_moving_average(df_result, windows)\n",
    "\n",
    "    df_result = calculate_rsi(df_filtered, df_result, windows)\n",
    "\n",
    "    df_result = calculate_macd(df_result)\n",
    "\n",
    "    df_result = calculate_bollinger_bands(df_filtered, df_result, windows)\n",
    "\n",
    "    df_result = calculate_obv(df_filtered, df_result)\n",
    "\n",
    "    df_result = df_result.select(*output_columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_to_delta(input_df, input_table, output_table):\n",
    "    def upsert_to_delta(batch_df, batch_id):\n",
    "\n",
    "        distinct_dates = [row['date'] for row in input_df.select('date').distinct().collect()]\n",
    "        distinct_dates_count = batch_df.select(\"date\").distinct().count()\n",
    "\n",
    "        if distinct_dates_count < 2:\n",
    "            df_filtered = read_prices(input_table,*distinct_dates)\n",
    "            df_filtered.cache()\n",
    "\n",
    "            latest_records = take_latest_row(output_table, df_streamed, \"daily\")\n",
    "            df_result = df_result.join(latest_records, on=\"symbol\", how=\"left\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            for single_date in distinct_dates:\n",
    "                df_single_date = input_df.filter(col('date') == single_date)\n",
    "                \n",
    "                \n",
    "\n",
    "        delta_table = DeltaTable.forName(spark, target_table)\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            batch_df.alias(\"source\"),\n",
    "            \"target.symbol = source.symbol AND target.date = source.date\"\n",
    "        ).whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n",
    "\n",
    "\n",
    "    query = (df.writeStream\n",
    "        .foreachBatch(upsert_to_delta)\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .outputMode(\"update\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start())\n",
    "    query.awaitTermination()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_streamed = readStream_prices()\n",
    "write_to_delta(df_streamed, input_table, output_table)\n",
    "\n",
    "latest_records = take_latest_row(output_table, df_streamed, \"daily\")\n",
    "df_result = df_result.join(latest_records, on=\"symbol\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3160f002-598c-493e-a9e7-3b3481d13053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM hive_metastore.dev_gold.daily_price_indicators\")\n",
    "pandas_df = df.limit(100).toPandas()\n",
    "display(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72b935aa-d14b-4861-8a98-02b157670139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_techMeasures_daily",
   "widgets": {
    "env": {
     "currentValue": "dev",
     "nuid": "10a4e55c-2959-49dd-a039-4843a2424b61",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter the environment in lower case",
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter the environment in lower case",
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
