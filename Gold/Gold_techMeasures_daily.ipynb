{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6fb1d02-cc8d-4738-911f-aa523b42fbcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/bartoszgardzinski1@gmail.com/DE_Databricks_1stpipe/utils/technical_indicators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad4b5af-767a-4152-ab1e-6e29c4b54c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(name=\"env\",defaultValue=\"\",label=\"Enter the environment in lower case\")\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    lit, avg, col, row_number, when, stddev, lag, sum as spark_sum, max as spark_max\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import date_sub, current_date\n",
    "import time\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "input_table = f\"{env}_silver.daily_price_aggregates\"\n",
    "output_table = f\"{env}_gold.daily_price_indicators\"\n",
    "output_columns = ['symbol', 'date', 'open', 'day_high', 'day_low', 'avg_price', 'volume', 'year', 'month', 'day', 'ma_12', 'ma_26', 'ma_60', 'ema_12', 'ema_26', 'ema_60', 'rsi_12', 'rsi_26', 'rsi_60', 'bb_upper_12', 'bb_lower_12', 'bb_width_12', 'bb_upper_26', 'bb_lower_26', 'bb_width_26', 'bb_upper_60', 'bb_lower_60', 'bb_width_60', 'macd', 'signal', 'macd_hist', 'obv']\n",
    "windows = [60,26,12]\n",
    "\n",
    "def read_prices(input_table):\n",
    "\n",
    "    # Calculate start date 90 days ago (Python side)\n",
    "    start_date = (datetime.today() - timedelta(days=90)).date()\n",
    "\n",
    "    # Extract year and month from start_date\n",
    "    start_year = start_date.year\n",
    "    start_month = start_date.month\n",
    "\n",
    "    # Also get current year and month\n",
    "    current_year = datetime.today().year\n",
    "    current_month = datetime.today().month\n",
    "\n",
    "    df_recent = spark.table(input_table) \\\n",
    "        .filter(\n",
    "            ( (col(\"year\") == start_year) & (col(\"month\") >= start_month) ) |\n",
    "            ( (col(\"year\") > start_year) & (col(\"year\") < current_year) ) |\n",
    "            ( (col(\"year\") == current_year) & (col(\"month\") <= current_month) )\n",
    "        )\n",
    "\n",
    "    df_filtered = df_recent.filter(col(\"date\") >= lit(start_date.strftime(\"%Y-%m-%d\")))\n",
    "    window_spec = Window.partitionBy(\"symbol\").orderBy(col(\"date\").desc())\n",
    "    df_latest = df_filtered.withColumn(\"rn\", row_number().over(window_spec))\n",
    "\n",
    "    return df_latest\n",
    "\n",
    "\n",
    "def take_latest_row(table_path, granulity):\n",
    "\n",
    "    if granulity == \"daily\":\n",
    "        datecol = \"date\"\n",
    "    elif granulity == \"hourly\":\n",
    "        datecol = \"timestamp\"\n",
    "\n",
    "    five_days_ago = date_sub(current_date(), 5)\n",
    "    df_recent = (\n",
    "        spark.read.table(table_path)\n",
    "        .filter(col(datecol) >= five_days_ago)  # Adjust to your date column\n",
    "    )\n",
    "\n",
    "    # Define window: partition by symbol, order by date descending\n",
    "    window_spec = Window.partitionBy(\"symbol\").orderBy(col(datecol).desc())\n",
    "\n",
    "    # Assign row number to get the latest record per symbol\n",
    "    df_latest = (\n",
    "        df_recent\n",
    "        .withColumn(\"rn\", row_number().over(window_spec))\n",
    "        .filter(col(\"rn\") == 1)\n",
    "        .drop(\"rn\")\n",
    "    )\n",
    "\n",
    "    for c in df_latest.columns:\n",
    "        if c != \"symbol\":\n",
    "            df_latest = df_latest.withColumnRenamed(c, f\"prev_{c}\")\n",
    "\n",
    "    return df_latest\n",
    "\n",
    "def write_indicators(df_result, path):\n",
    "    target_table = DeltaTable.forName(spark, output_table)\n",
    "\n",
    "    target_table.alias(\"target\").merge(\n",
    "    source=df_result.alias(\"source\"),\n",
    "    condition=\"target.symbol = source.symbol AND target.date = source.date\"\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "\n",
    "\n",
    "df_filtered = read_prices(input_table)\n",
    "df_filtered.cache()\n",
    "\n",
    "df_result = df_filtered.filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "latest_records = take_latest_row(output_table, \"daily\")\n",
    "df_result = df_result.join(latest_records, on=\"symbol\", how=\"left\")\n",
    "\n",
    "start = time.time()\n",
    "df_result = moving_average(df_filtered, df_result, windows)\n",
    "df_result.count()\n",
    "print(f\"moving_average took {time.time() - start:.3f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "df_result = exponential_moving_average(df_result, windows)\n",
    "df_result.count()\n",
    "print(f\"exponential_moving_average took {time.time() - start:.3f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "df_result = calculate_rsi(df_filtered, df_result, windows)\n",
    "df_result.count()\n",
    "print(f\"calculate_rsi took {time.time() - start:.3f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "df_result = calculate_macd(df_result)\n",
    "df_result.count()\n",
    "print(f\"calculate_macd took {time.time() - start:.3f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "df_result = calculate_bollinger_bands(df_filtered, df_result, windows)\n",
    "df_result.count()\n",
    "print(f\"calculate_bollinger_bands took {time.time() - start:.3f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "df_result = calculate_obv(df_filtered, df_result)\n",
    "df_result.count()\n",
    "print(f\"calculate_obv took {time.time() - start:.3f} seconds\")\n",
    "\n",
    "\n",
    "df_result = df_result.select(*output_columns)\n",
    "\n",
    "write_indicators(df_result, output_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3160f002-598c-493e-a9e7-3b3481d13053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM hive_metastore.dev_gold.daily_price_indicators\")\n",
    "pandas_df = df.limit(100).toPandas()\n",
    "display(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72b935aa-d14b-4861-8a98-02b157670139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_techMeasures_daily",
   "widgets": {
    "env": {
     "currentValue": "dev",
     "nuid": "10a4e55c-2959-49dd-a039-4843a2424b61",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter the environment in lower case",
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter the environment in lower case",
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
